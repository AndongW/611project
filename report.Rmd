---
title: "BIOS611 Final Report"
author: "Anthony Wang"
output: html_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Data Description

The datasets used in this analysis are downloaded from the following kaggle page:

https://www.kaggle.com/datasets/kauvinlucas/30000-albums-aggregated-review-ratings

Please follow the link to see more detailed description of the dataset. I will provide a brief overview. 

There are three original data files, including album_ratings.csv (I will refer to it as "album ratings data"), and a test.csv and train.csv in the album_reviews folder. The test.csv and train.csv files were split from one dataset for the purpose of model building, so I will refer to them together as "album reviews data". 

The album ratings data have 16 variables, including information on album title, artist name, release date, format, label company name, genre, and numerical ratings and numbers of reviews from both critics and users on Metacritic (www.metacritic.com) and AOTY (www.albumoftheyear.org), which are both reputable and popular platforms for music critics and music consumers to share their ratings and reviews on albums. The album ratings data has more than 30,000 albums recorded, however, some albums have missing information for certain variables. 

The album reviews data have 6 variables, including information on album title, artist name, media source of rating/review, numeric rating and textual review given by the media, and the reception classification of the album, which simply categorizes the numerical ratings into 5 strata: Acclaim (81-100), Favorable (61-80), Mixed (41-60), Unfavorable (21-40), and Dislike (0-20). Some of the most popular media sources from the album reviews are: Allmusic, Pitchfork, Popmatters, Uncut, Q magazine, etc. The album reviews data has recorded more than 166,000 album-media source parings, however, no media source have reviewed all the albums. 


# Data Preprocessing

For my analysis, I used both album ratings data and album reviews data. Here is a brief overview of what I did to prepare the data for analysis. 

In rate_data_prep.R, I preprocessed the album ratings data. I removed duplicate album entries. Then, I categorized the albums into "Pop", "Rock", and "Neither" based on their genre. Then, I created an adjusted score for each of Metacritic and AOTY, by combining the critic scores and user scores with weights based on the number of critic reviews and user reviews. 

In rev_data_prep.R, I first combined test and train sets of the album reviews data. I then removed the text field and reception variable, and any duplicate albums-media source parings, to prepare for pivoting the data. Then, I pivoted the data wider, so that each row of the transformed dataset represents one distinct album and has information on the album's ratings from each source (NA for a specific source variable, if the album did not recieve rating from that source). In the end, the dataset has 133 columns, even though columns represented by the not-so-popular media sources are mostly empty. 

In data_merge.R, I merged (inner-join) the two preprocessed datasets by album, specifically album title and artist name. 

# Visualization and Analysis

## Pop vs. Rock

First, I make some overlaid density plots comparing distributions of adjusted, critic only, and user only scores between pop and rock albums. 

![](./figures/poprock.png){width=100%}

In general, the distributions under all settings are similar in shape. This means pop and rock albums are similarly received in general. 

For AOTY, all adjusted, critic, and user scores peaked at around 75 for both pop and rock albums. 

For Metacritic, pop and rock albums have similar distributions, however, it appears that users tend to give higher scores to albums compared to critics. I explore this nuance by making more overlaid density plots, this time focusing on comparing the critic scores and user scores.

## Critic vs User

![](./figures/criticuser.png){width=100%}

The Metacritic plots confirms the difference between critic and user scores for both pop and rock albums in a similar fashion. Metacritic users tend to give higher scores to albums compared to Metacritic critics. It is interesting that AOTY data does not display the same pattern. This may have to do with the differing targeted audience between the two media sites. 

It is worth noting that for AOTY, users tend give more consistent scores compared to critics, as evident in the taller peak, skinner peak. This may be because the users are more easily influenced by other users' reviews and ratings, explained by the herd behavior.

## Scores Through the Years



